# Data Lab

*Assigned: Thursday, February 1st*

*Due: Saturday February 11th, 11:59PM EST*

The goal of this lab is for you to experiment with working with data
in various degrees of structure.  

You will work with a dataset of Tweets encoded in multiple ways to compute
some summary information and reflect on the pros and cons of each
approach.

**The spirit** of this lab is to get you to think about different trade-offs
that unstructured and structured data provide to you, as the developer.

# Setup

See the [setup instructions](./SETUP.md)


### Step 1: Look at some JSON-encoded Tweets

First decompress the Tweets data file:

    gzip -d twitter.json.gz

`twitter.json` contains a JSON-encoded tweet on each line.  Check out
[twitter's documentation](https://dev.twitter.com/docs/platform-objects/tweets)
for information about the tweets or simply read the file.  Take a look
at this file.  *You won't be using it directly, but we've written some
scripts to process it for use in future steps.*

By reading `twitter.json`, you've completed Step 1!

In the following steps, you will use three systems to perform data
analysis: Protocol Buffers, a relational database called SQLite, and
MongoDB.  Using each of the three systems, you're going to answer the
following three questions:

1. Find the number of deleted messages in the dataset.
2. Find the number of tweets that are replies to another tweet.
3. Find the five user IDs (field name: `uid`) that have tweeted the most.

<!-- 4. Find the names of the top five places by number of tweets.  (Tweets may have a "place" attribute that describes where the tweet is from.  If it doesn't you can ignore the tweet).) -->

### Step 2: Analyses using Protocol Buffers

In this step, you will use protocol buffers to perform the analyses.

For details about protocol buffers, [read this
page](https://developers.google.com/protocol-buffers/docs/overview).
In essence, it's helpful within a company to know the schema of data
and messages being stored and transmitted by various services.
Protocol buffers, Thrift, and Avro are all projects that help define
schemas and compact/efficient serialization formats for your data.
None of these tools help you process the data with query languages and
data processing frameworks: those are built on top of these libraries.

The protocol buffer definitions are found in `twitter.proto` and the
data files are in `twitter.pb`.

We generated the protocol buffer's python bindings by running:

    protoc twitter.proto --python_out=.
    
This should generate a `twitter_pb2.py` that you can `import` in your
python scripts.

`twitter.pb` is generated by calling `python encode.py`, but you do
not need to re-create the file.  Note that this script does not
include all fields: we serialized only the subset of fields that are
necessary for answering the questions.

*Perform the three analyses from Step 1 using protocol buffers.  Keep a
copy of your code and the answers. You will have to write some code to
process the protocol buffer-encoded data.  There are [official
C++/Java/Python
libraries](https://developers.google.com/protocol-buffers/docs/reference/overview)
you can use, as well as [other language implementations in the
third-party
listings](https://code.google.com/p/protobuf/wiki/ThirdPartyAddOns).*


### Step 3: Analyses on database records

In this step, you will be working with the twitter data encoded as a
sqlite3 database file.  Run the following to create the database (it'll take a while).

```
$ python createdb.py
```

The schemas are defined in `twitter.ddl`. Note that this schema does not
include all fields: we includes only the subset of fields that are
necessary for answering the questions.

Start a sqlite3 prompt by typing:

    sqlite3 twitter.db

For SQL help, refer to the fantastic [sqlite documentation](http://www.sqlite.org/docs.html)
and [postgresql's documentation](http://www.postgresql.org/docs/).

*Perform the three analyses listed in Step 1 using sqlite.  Keep a copy
 of your code and the answers.*




### Step 4: Analyses in MongoDB

In this step, we will import and query the JSON data we've collected
in MongoDB.  First, let's get the data into Mongo:

    mongoimport -d lab2 -c tweets twitter.json

This will, in the database `lab2`, create a collection called `tweets`
from the JSON blobs inside `twitter.json`.

To access the `lab2` database, type

    mongo lab2

Refer to Mongo's detailed [query language documentation](http://docs.mongodb.org/manual/reference/method/db.collection.find/#db.collection.find) for help.

*Perform the three analyses listed in Step 1 using MongoDB. Keep a copy of your code and the answers.*

**Note:** 

1. You can use any programming language of your choice to interact with MongoDB or simply run queries from within the Mongo CLI. 
2. We want you to write MongoDB queries, so any solutions that load all of the data into a programming language and performs processing similar to Step 2 is not accepted
  2. If you are not using the Mongo CLI, then you should not call the entire collection, like doing docs = "SELECT * FROM table", and then perform filtering on docs to get the solution. In other words, your Mongo query should be responsible for filtering / aggregating the data rather than your programming language. 
  3. It is required of you to query the database that extracts the complete solution or atleast a partial solution followed by some processing to eventually get the complete solution.

### Step 5: Using a New Dataset (Optional)

This is an optional task. In this step, you will learn how to design schema given a dataset and execute join queries.  

Use WeatherDataset.csv file to create your own schema and add it to existing database as described below. You have to write a query that fetches all (tweets, temperature) pairs originating in the 'US' when weather condition was 'Clear' in the year 2013 (weather and twitter dataset both have attribute "country_code" which can be used to get the desired result). Complete the above task as follows: -

1. Write a python script using the twitter protocol buffer data and the weather csv file
2. Now, use sqlite for the above task: -

```
		$ sqlite3 twitter.db
		sqlite> <write create table statement using WeatherDataset.csv file> (Refer twitter.ddl file to learn how to design schema) 
		sqlite> .mode csv
		sqlite> .import WeatherDataset.csv <table name> (type ".help" for commands and help)
		sqlite> <write sql query to complete the required task>
```



# Submission

### Reflection Questions

One or two sentences answers are sufficient

1. Read the schema and protocol buffer definition files.  What are the main differences between the two?  Are there any similarities?
1. Describe one question that would be easier to answer with protocol buffers than via a SQL query.
1. Describe one question that would be easier to answer with MongoDB than via a SQL query.
1. Describe one question that would be easier to answer via a SQL query than using MongoDB.
1. What fields in the original JSON structure would be difficult to convert to relational database schemas?
1. In terms of lines of code, when did various approaches shine?  Think about the challenges of defining schemas, loading and storing the data, and running queries.
1. What other metrics (e.g., time to implement, code redundancy, etc.) can we use to compare these different approaches?  Which system is better by those measures?
1. How long did this lab take you?  We want to make sure to target future labs to not take too much of your time.

### Handing in your work

1. Create a PDF file containing: 
  * your name and your uni
  * the results of the three analyses from Step 1 as run on the three systems in Steps 2, 3, and 4, 
  * the brief responses to the reflection questions 
  * all the code for all the steps that **you've** written. Please don't upload the same files that we've provided for this assignment.
 Â * other questions you may have
2. Submit on Courseworks.

### Feedback (optional, but valuable)

If you have any comments about this lab, or any thoughts about the class so far, we would greatly appreciate them.  Your comments will be strictly used to improve the rest of the labs and classes and have no impact on your grade. 

Some questions that would be helpful:

* Is the lab too difficult or too easy?  
* Did you look forward to any exercise that the lab did not cover?
* Which parts of the lab were interesting or valuable towards understanding the material?
* How is the pace of the course so far?
